{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433c7237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pytorch_msssim import ms_ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from models.tinylic import TinyLIC\n",
    "from quant_int import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d94c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d9eca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psnr(a, b):\n",
    "    mse = torch.mean((a - b)**2).item()\n",
    "    return -10 * math.log10(mse)\n",
    "\n",
    "def compute_msssim(a, b):\n",
    "    return ms_ssim(a, b, data_range=1.).item()\n",
    "\n",
    "def filesize(filepath: str) -> int:\n",
    "    if not Path(filepath).is_file():\n",
    "        raise ValueError(f'Invalid file \"{filepath}\".')\n",
    "    return Path(filepath).stat().st_size\n",
    "\n",
    "def pad(x, p=2 ** 6):\n",
    "    h, w = x.size(2), x.size(3)\n",
    "    H = (h + p - 1) // p * p\n",
    "    W = (w + p - 1) // p * p\n",
    "    padding_left = (W - w) // 2\n",
    "    padding_right = W - w - padding_left\n",
    "    padding_top = (H - h) // 2\n",
    "    padding_bottom = H - h - padding_top\n",
    "    return F.pad(\n",
    "        x,\n",
    "        (padding_left, padding_right, padding_top, padding_bottom),\n",
    "        mode=\"constant\",\n",
    "        value=0,\n",
    "    )\n",
    "\n",
    "def crop(x, size):\n",
    "    H, W = x.size(2), x.size(3)\n",
    "    h, w = size\n",
    "    padding_left = (W - w) // 2\n",
    "    padding_right = W - w - padding_left\n",
    "    padding_top = (H - h) // 2\n",
    "    padding_bottom = H - h - padding_top\n",
    "    return F.pad(\n",
    "        x,\n",
    "        (-padding_left, -padding_right, -padding_top, -padding_bottom),\n",
    "        mode=\"constant\",\n",
    "        value=0,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2d782e2",
   "metadata": {},
   "source": [
    "## Quantize FP32 to INT8 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17a8494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='running parameters',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "# general parameters for data and model\n",
    "parser.add_argument('--seed', default=1005, type=int, help='random seed for results reproduction')\n",
    "parser.add_argument('--name', default=datetime.now().strftime('%Y-%m-%d_%H_%M_%S'), type=str, help='result dir name')\n",
    "parser.add_argument('--save', default=True, help='save quantized model')\n",
    "parser.add_argument('--fp32_name',default='tinylic', help='fp32_model_path')\n",
    "\n",
    "# quantization parameters\n",
    "parser.add_argument('--n_bits_w', default=8, type=int, help='bitwidth for weight quantization')\n",
    "parser.add_argument('--channel_wise', action='store_true', help='apply channel_wise quantization for weights')\n",
    "parser.add_argument('--act_quant', default=True, help='apply activation quantization')\n",
    "parser.add_argument('--test_before_calibration', default=True, type=bool, help='test_before_calibration')\n",
    "parser.add_argument('--sym', default=True, help='symmetric reconstruction')\n",
    "\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3de327ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False  # may slow\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be8db12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    sum_psnr = 0.0\n",
    "    sum_msssim = 0.0\n",
    "    sum_bpp = 0.0\n",
    "    \n",
    "    lambda_rd = torch.tensor([0.0007]).to(device)\n",
    "    img_num = 1\n",
    "    for i in range(1):\n",
    "        img = Image.open('./data/Kodak/kodim'+str(i+1).zfill(2)+'.png').convert('RGB')\n",
    "        x = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        p = 64\n",
    "        h, w = x.size(2), x.size(3)\n",
    "        x_pad = pad(x, p)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            out_enc = model.compress(x_pad, lambda_rd)\n",
    "            out = model.decompress(out_enc[\"strings\"], out_enc[\"shape\"], lambda_rd)\n",
    "            rec = crop(out['x_hat'], (h, w))\n",
    "\n",
    "        num_pixels = x.size(0) * x.size(2) * x.size(3)\n",
    "        bpp = sum(len(s[0]) for s in out_enc[\"strings\"]) * 8.0 / num_pixels\n",
    "\n",
    "\n",
    "        sum_psnr += compute_psnr(x, rec)\n",
    "        sum_msssim += compute_msssim(x, rec)\n",
    "        sum_bpp += bpp\n",
    "\n",
    "    return sum_psnr/img_num, sum_msssim/img_num , sum_bpp/img_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3057e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(qnn, args):\n",
    "    device = next(qnn.parameters()).device\n",
    "    img = Image.open('./data/Kodak/kodim'+str(23).zfill(2)+'.png').convert('RGB')\n",
    "    x = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    p = 64\n",
    "    h, w = x.size(2), x.size(3)\n",
    "    x_pad = pad(x, p)\n",
    "    lambda_rd = torch.tensor([0.0005]).to(device)\n",
    "    \n",
    "    # Initialize weight quantization parameters\n",
    "    qnn.set_quant_state(True, args.act_quant)\n",
    "    init_start = time.time()\n",
    "   #  _ = qnn(x_pad)\n",
    "    _ = qnn(x_pad, lambda_rd)\n",
    "    init_time = time.time() - init_start\n",
    "    logging.info('generate quantized model time: {}'.format(init_time))\n",
    "\n",
    "    return qnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc61f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_int8(args, fp32_name, output_dir, log_dir):\n",
    "    \"\"\"\n",
    "    Quantize a FP32 Models to INT8 Models;\n",
    "    Based on Post-Training-Quantization (PTQ):\n",
    "        weight: channel-wise quantization;\n",
    "        activation: layer-wise quantization(faster than channel-wise);\n",
    "        \n",
    "    Quantized Modules:\n",
    "            hyper encoder: h_a\n",
    "            hyper decoder: h_s\n",
    "            entropy coding modules: entropycc_transforms, sc_transformers, entropy_parameters\n",
    "    \n",
    "    Args Input\n",
    "        :param fp32_name: the name of FP32 Models\n",
    "        :param output_dir: the path to INT8 Models\n",
    "        :param log_dir: the path to logger \n",
    "    \n",
    "    Args Output\n",
    "        :INT8 Models, saved in ./results/...\n",
    "    \"\"\"\n",
    "    \n",
    "    # load model \n",
    "    model = TinyLIC(model_size = \"80M\")\n",
    "    snapshot = torch.load('./pretrained/'+fp32_name+'.pth.tar', map_location=device)['state_dict']\n",
    "    model.load_state_dict(snapshot, strict=False)\n",
    "    model.update(force=True)\n",
    "    model.entropy_bottleneck.update()\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    if args.test_before_calibration:\n",
    "        logging.info('Full-precision model: psnr= {:.2f}; ms-ssim={:.4f}; bpp= {:.3f}'.format(*validate_model(model)))\n",
    "\n",
    "    # build quantization parameters\n",
    "    wq_params = {'n_bits': args.n_bits_w, 'channel_wise': True, 'symmetric': False, 'scale_method': 'max'}\n",
    "    aq_params = {'channel_wise': False, 'symmetric': False, 'scale_method': 'max', 'leaf_param': True}\n",
    "    qnn = QuantModel(model=model, weight_quant_params=wq_params, act_quant_params=aq_params)\n",
    "    qnn.to(device)\n",
    "    qnn.eval()\n",
    "    # logging.info('quantized model architecture: {}'.format(qnn))\n",
    "\n",
    "    # qnn.disable_network_output_quantization()\n",
    "    qnn = generator(qnn, args)\n",
    "\n",
    "    qnn.set_quant_state(weight_quant=True, act_quant=True)\n",
    "    logging.info('INT8: psnr= {:.2f}; ms-ssim={:.4f}; bpp= {:.3f}'.format(*validate_model(qnn)))\n",
    "\n",
    "    if args.save:\n",
    "        logging.info('save quantized model in {}'.format(output_dir))\n",
    "        torch.save(qnn.state_dict(), \"{}/INT8.pth\".format(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48151fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_int8(fp32_name):\n",
    "    args.fp32_name = fp32_name\n",
    "    output_dir, log_dir = init_lic(args)\n",
    "\n",
    "    seed_all(args.seed)\n",
    "    setup_logger(log_dir + '/' + time.strftime('%Y%m%d_%H%M%S') + '.log')\n",
    "\n",
    "    logging.info('[PID] %s'%os.getpid())\n",
    "    msg = f'======================= TinyLIC ======================='\n",
    "    logging.info(msg)\n",
    "\n",
    "    quantize_int8(args, fp32_name, output_dir, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8b3d7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-26 03:59:28,661 [INFO ]  Logging file is ./results/tinylic/logs/20230926_035928.log\n",
      "2023-09-26 03:59:28,662 [INFO ]  [PID] 31946\n",
      "2023-09-26 03:59:28,665 [INFO ]  ======================= TinyLIC =======================\n",
      "2023-09-26 03:59:35,484 [INFO ]  Full-precision model: psnr= 24.28; ms-ssim=0.9060; bpp= 0.221\n",
      "2023-09-26 03:59:40,886 [INFO ]  generate quantized model time: 5.162351131439209\n",
      "2023-09-26 03:59:41,176 [INFO ]  INT8: psnr= 23.76; ms-ssim=0.8914; bpp= 0.195\n",
      "2023-09-26 03:59:41,871 [INFO ]  save quantized model in ./results/tinylic/outputs\n"
     ]
    }
   ],
   "source": [
    "fp32_name = 'tinylic'\n",
    "\n",
    "main_int8(fp32_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54aed98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c5f0009",
   "metadata": {},
   "source": [
    "## Quantize FP32 to FP16 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbc012cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_fp16(fp32_name):\n",
    "    \"\"\"\n",
    "    Quantize a FP32 Models to FP16 Models;\n",
    "    Based on torch.half to convert;\n",
    "    \n",
    "    Args Input\n",
    "    :param fp32_name: the name of FP32 Models\n",
    "    \n",
    "    Args Output\n",
    "    :FP16 Models, saved in ./results/...\n",
    "    \"\"\"\n",
    "    \n",
    "    # load FP32 Models and update \n",
    "    model = TinyLIC()\n",
    "    snapshot = torch.load('./pretrained/'+fp32_name+'.pth.tar', map_location=device)['state_dict']\n",
    "    model.load_state_dict(snapshot, strict=False)\n",
    "    model.update(force=True)\n",
    "    \n",
    "    # quantize to FP16\n",
    "    model = model.half().to(device).eval()\n",
    "    \n",
    "    # save model, saving state_dict and full_model are both acceptable.\n",
    "    # torch.save({'state_dict': model.state_dict()}, fp16_dir)\n",
    "    torch.save(model, './results/'+fp32_name+'/outputs/FP16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a4fa018",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_name = 'tinylic'\n",
    "\n",
    "quant_fp16(fp32_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fdceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
